{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d0ce4b",
   "metadata": {},
   "source": [
    "## Week 3 Assignment\n",
    "\n",
    "#### Task\n",
    "\n",
    "In the course material, a simple convolutional neural network is built, trained and tested to solve the multiclass classification task presented by the CIFAR-10 dataset. To improve the accuracy, you should experiment with pre trained models. Follow the instructions in Chollet's book \"Deep Learning with Python\", 2nd edition, Chapter 8, pp. 225-231: Feature extraction with a pre trained model. Pick one of the pre trained models available with Keras, and discard the Dense classifier top. You should only use the convolution base to preprocess the original images to a new representation, using its predict method. For this modified input data, you should build a simple fully connected classifier, train it, and test it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=77)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e503f2",
   "metadata": {},
   "source": [
    "#### Model Creation and Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daacf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Resizing\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications import VGG16\n",
    "\n",
    "\n",
    "# Define feature extractor\n",
    "base_model = VGG16(include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define a batch processing\n",
    "def process_images_in_batches(images, labels, batch_size=32):\n",
    "    resize_layer = Resizing(224, 224)\n",
    "    features_list = []\n",
    "    total_batches = len(images) // batch_size + (1 if len(images) % batch_size > 0 else 0)\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i + batch_size].astype('float32')\n",
    "        \n",
    "        # Resize\n",
    "        batch_resized = resize_layer(batch_images)\n",
    "        \n",
    "        # Preprocess for VGG16\n",
    "        batch_preprocessed = preprocess_input(batch_resized)\n",
    "        \n",
    "        # Extract features\n",
    "        batch_features = base_model.predict(batch_preprocessed, verbose=1)\n",
    "        \n",
    "        # Flatten features\n",
    "        batch_features_flat = batch_features.reshape(batch_features.shape[0], -1)\n",
    "        features_list.append(batch_features_flat)\n",
    "        \n",
    "        print(f\"Processed batch {i//batch_size + 1}/{total_batches}\")\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    features = np.concatenate(features_list)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Process training, validation and test data\n",
    "print(\"Processing training data...\")\n",
    "train_features, train_labels = process_images_in_batches(x_train, y_train)\n",
    "print(\"Processing validation data...\")\n",
    "val_features, val_labels = process_images_in_batches(x_val, y_val)\n",
    "print(\"Processing test data...\")\n",
    "test_features, test_labels = process_images_in_batches(x_test, y_test)\n",
    "\n",
    "# Print shapes to confirm processing worked\n",
    "print(f\"Train features shape: {train_features.shape}, labels shape: {train_labels.shape}\")\n",
    "print(f\"Validation features shape: {val_features.shape}, labels shape: {val_labels.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}, labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ece8b",
   "metadata": {},
   "source": [
    "#### Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a258c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add this code cell after your feature extraction code\n",
    "\n",
    "# Create a directory to save features\n",
    "os.makedirs(\"saved_features\", exist_ok=True)\n",
    "\n",
    "# Save the processed features\n",
    "np.save(\"saved_features/train_features.npy\", train_features)\n",
    "np.save(\"saved_features/train_labels.npy\", train_labels)\n",
    "np.save(\"saved_features/val_features.npy\", val_features)\n",
    "np.save(\"saved_features/val_labels.npy\", val_labels)\n",
    "np.save(\"saved_features/test_features.npy\", test_features)\n",
    "np.save(\"saved_features/test_labels.npy\", test_labels)\n",
    "\n",
    "print(\"Features and labels saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38013a",
   "metadata": {},
   "source": [
    "#### Load saved features from hard drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d30fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if saved files exist\n",
    "if os.path.exists(\"saved_features/train_features.npy\"):\n",
    "    print(\"Loading saved features...\")\n",
    "    train_features = np.load(\"saved_features/train_features.npy\")\n",
    "    train_labels = np.load(\"saved_features/train_labels.npy\")\n",
    "    val_features = np.load(\"saved_features/val_features.npy\")\n",
    "    val_labels = np.load(\"saved_features/val_labels.npy\")\n",
    "    test_features = np.load(\"saved_features/test_features.npy\")\n",
    "    test_labels = np.load(\"saved_features/test_labels.npy\")\n",
    "    \n",
    "    print(f\"Train features shape: {train_features.shape}, labels shape: {train_labels.shape}\")\n",
    "    print(f\"Validation features shape: {val_features.shape}, labels shape: {val_labels.shape}\")\n",
    "    print(f\"Test features shape: {test_features.shape}, labels shape: {test_labels.shape}\")\n",
    "else:\n",
    "    print(\"No saved features found. Run the feature extraction first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13133f0f",
   "metadata": {},
   "source": [
    "#### Dense layers build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d92c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "# Define layers\n",
    "inputs = Input(shape=(train_features.shape[1],))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Add inputs and outputs to model\n",
    "dense_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model and print summary\n",
    "dense_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Callback functions\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.2, patience=5)\n",
    "\n",
    "\n",
    "\n",
    "# One-hot encode the labels\n",
    "train_labels_one_hot = to_categorical(train_labels, num_classes=10)\n",
    "val_labels_one_hot = to_categorical(val_labels, num_classes=10)\n",
    "\n",
    "# Train model\n",
    "history = dense_model.fit(\n",
    "    train_features, train_labels_one_hot,\n",
    "    validation_data=(val_features, val_labels_one_hot),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a2ea3",
   "metadata": {},
   "source": [
    "#### Evaluation and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss graph\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy graph\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# One hot encode test labels\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "\n",
    "# Test model with test data\n",
    "test_loss, test_acc = dense_model.evaluate(test_features, test_labels_one_hot, verbose=1)\n",
    "print(f\"Feature extractor model test loss: {test_loss:.2f}\")\n",
    "print(f\"Feature extractor model test accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b2649",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "I chose VGG16 pre trained model for my base model to this assignment, because it is top 5 model in accuracy and it has ~138 million parameters and relatively small inference time per step when calculated with CPU (69.5 ms).\n",
    "\n",
    "- First test gave me test loss of 0.55 and test accuracy of 85% with two Dense layers using relu and two Dropout layers.\n",
    "- Second test gave me test loss of 0.38 and test accuracy of 88%. I only added Batchnormalization layer to Dense model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
